{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e717f3-d635-493b-bca0-338aab2fa428",
   "metadata": {},
   "source": [
    "# Welcome to the qBraid presentation on techinques to embed classical images into quantum registers !\n",
    "\n",
    "<img src=\"./_images/scheme.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5c8dd-5b44-4cde-9100-ca04e490bcd2",
   "metadata": {},
   "source": [
    "Quantum computing (QC) is acknowledged to have significant speedups in solving intractable problems for classical computation by utilizing quantum mechanical phenmena such as entanglement and superposition. Current development of QCs have allowed for successful manipulation of approximately several dozen to a hundred noisy qubits, placing progress of various quantum architectures to be in the Noisy Intermediate Scale Quantum (NISQ) computing era. A promising area of research is Quantum Machine Learning (QML) which is known to provide atypical calculation patterns as well as relatively strong performances with low parameter usage compared to their classical counterparts.\n",
    "\n",
    "\n",
    "Today, we'll be using qBraid to understand how to embed classical images into quantum registers, the current embedding implementations, and the current limitations and problems at hand. \n",
    "\n",
    "Agenda\n",
    "\n",
    "- Getting Started with qBraid\n",
    "- Classical implementation\n",
    "- Quantum Embedding/Implementation\n",
    "- qBraid SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933f857-4545-46e1-b87f-367ef53f240b",
   "metadata": {},
   "source": [
    "## Getting Started on qBraid\n",
    "\n",
    "### Step 1.\n",
    "If you haven't already done so, please make a qBraid account and add the access code `EHNU6626` on the account.qbraid.com/account-details page. \n",
    "\n",
    "\n",
    "### Step 2.\n",
    "Then click on the `Launch on qBraid` button in the README.md of this repository. The button will automatically clone the repository and take you to your *new* qBraid Lab integrated development environment. \n",
    "\n",
    "### Step 3.\n",
    "Finally, install the qBraid-SDK environment via the qBraid Lab Environment Manager. On Lab you should see the `ENVS` icon on the right. The qBraid Lab Environment Manager is a robust package and virtual environment management system provided to qBraid end-users through a simple, intuitive graphical user interface. To expand the Environment Manager sidebar, click on Envs in the upper-right of the Lab console. My Environments are your currently installed environments. The qBraid Default environment and Microsoft Q# environment are installed by default.\n",
    "\n",
    "Install environment\n",
    "1. In the Environment Manager sidebar, click Add to view the environments available to install.\n",
    "\n",
    "Choose the qBraid SDK, expand its panel, and click Install.\n",
    "\n",
    "<img src=\"./_images/env_install.png\">\n",
    "\n",
    "3. Once the installation has started, the pannel is moved to the My Environments tab. Click Browse Environments to return to the My Environments tab and view its progress.\n",
    "\n",
    "\n",
    "\n",
    "Browse Environments to return to the My Environments tab and view its progress.\n",
    "\n",
    "<img src=\"./_images/env_installing.png\">\n",
    "\n",
    "4. When the installation is complete, the environment panel’s action button will switch from Installing… to Activate. Clicking Activate creates a new ipykernel, see Kernels for more.\n",
    "\n",
    "<img src=\"./_images/kernel_activate.png\">\n",
    "\n",
    "To uninstall the environment, click on More, and then Uninstall. Learn more about qBraid Lab Environment Manager [here](https://qbraid-qbraid.readthedocs-hosted.com/en/stable/lab/environments.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a0699-bdc0-4e89-9859-5ca716d4bb8c",
   "metadata": {},
   "source": [
    "## Activate the qBraid SDK kernel\n",
    "Under My Environments, choose the environment, and expand its pannel. Click Activate to activate the environment and create an associated ipykernel.\n",
    "\n",
    "<img src=\"./_images/kernel_activate.png\">\n",
    "\n",
    "Switch notebook kernel\n",
    "In the Launcher tab, under Notebooks, clicking on an ipykernel associated with an activated environment will automatically launch a Jupyter notebook (.ipynb file) using that kernel. In the upper-right of the newly created notebook, you can see which kernel is in use.\n",
    "\n",
    "<img src=\"./_images/kernel_nb.png\">\n",
    "\n",
    "Clicking on the name of the current kernel, as circled above, will open the kernel selector, and allow you switch to any other active kernel.\n",
    "\n",
    "<img src=\"./_images/kernel_switch.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b10931-f6eb-42eb-a2d0-ec03dffbacf1",
   "metadata": {},
   "source": [
    "Next we'll install tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d38e2d0-7934-443f-92b1-6e499166a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (1.23.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (1.48.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: setuptools in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (58.1.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (0.28.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (22.11.23)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.18.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.6.15.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorboard in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (1.23.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.9/site-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (58.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (1.3.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (1.48.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (3.19.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (4.11.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-plot in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (0.3.7)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from scikit-plot) (3.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from scikit-plot) (1.1.3)\n",
      "Requirement already satisfied: scipy>=0.9 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from scikit-plot) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.10 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from scikit-plot) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (4.37.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from scikit-learn>=0.18->scikit-plot) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install tensorboard\n",
    "%pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d3280d-0314-42ca-86b7-17a04a29c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from qbraid import circuit_wrapper\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# CONSTANTS\n",
    "NUM_EXAMPLES=500\n",
    "os.environ['TENSORBOARD_BINARY'] = '/home/jovyan/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/bin/tensorboard'\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed189f-4ba5-4143-b395-1d7c650e0cb5",
   "metadata": {},
   "source": [
    "\n",
    "# MNIST\n",
    "The MNIST (Modified National Institute of Standards and Technology) database contains 70,000 28 x 28 images of handwritten digits from 0-9 and is seminal to machine learning. The MNIST handwritten dataset is the “Hello World” implementation for machine learning, and the dataset is used as a worldwide machine learning benchmark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a33af1-372b-46cc-aee2-e43685d30e96",
   "metadata": {},
   "source": [
    "## Starting off with a classical implementation\n",
    "We will first load the data and apply a classical CNN (Convolutional Neural Network) to understand the mechanics of image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401bb9cd-a212-4b2c-91b7-7a692f0e02f1",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "We load the data from tensorflow, a machine learning package developed by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad42372-8ae2-4c5d-b068-f9ea0d63bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images from [0,255] to the [0.0,1.0] range.\n",
    "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
    "\n",
    "print(\"Number of original training examples:\", len(x_train))\n",
    "print(\"Number of original test examples:\", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f320bfe-4090-435c-95d4-ec23dfd9d3a9",
   "metadata": {},
   "source": [
    "We one hot encode the categories into ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2036167-f139-41e1-99d7-8ecb2e98bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = tf.one_hot(y_train,10)\n",
    "y_test_onehot = tf.one_hot(y_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc61848-7164-49e0-959e-a3b73019d1d1",
   "metadata": {},
   "source": [
    "Let's plot the images to see what we're going to be classifying and embedding into quantum circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf0a0c-bb0a-485d-a54c-d8c1f9016542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41eeb71-4de8-4b07-8de6-4f228e34c79f",
   "metadata": {},
   "source": [
    "### Run a CNN (convolutional neural network)\n",
    "In the following cells we run a simple CNN which has 2 convolutional layers and will classify, on average, to 98%. A convolutional layer is an integral transform which detects certain features using a filter to pass over the image.\n",
    "<img src=\"./_images/convolutionalfilter.gif\">\n",
    "\n",
    "\n",
    "You can learn more about classical CNNs here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27619c79-1975-45cb-a80d-a1cb1c8481b7",
   "metadata": {},
   "source": [
    "To gather data on our model per epoch, we will include tensorboard and the model checkpoint callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc8132-c808-4f07-8089-3bca6de73604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard callback\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Specify Folders\n",
    "current_time = str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "train_log_dir = 'logs/tensorboard/' + current_time\n",
    "test_log_dir = 'logs/tensorboard/test/' + current_time\n",
    "path_for_checkpoint_callback = 'logs/summary/'+current_time\n",
    "\n",
    "# Create callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=train_log_dir, histogram_freq=1, profile_batch=3\n",
    "            )\n",
    "\n",
    "# Model Checkpoint callback\n",
    "model_ckpt =  tf.keras.callbacks.ModelCheckpoint(\n",
    "                path_for_checkpoint_callback, save_weights_only=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971f133-06e2-45b9-9a8c-1b45d5513d9d",
   "metadata": {},
   "source": [
    "The classical model is a sequential CNN for 10 class multiclassification. The model contains  1,199,882 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b37b50-0f22-48d7-bbb9-c6ed15cdc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classical_model():\n",
    "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, [3, 3], activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, [3, 3], activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "cmodel = create_classical_model()\n",
    "cmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(0.002),\n",
    "               metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "cmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c617d1a-0e8f-4c14-90a0-c4cb3297126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel.fit(x_train,\n",
    "          y_train_onehot,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test[:1000], y_test_onehot[:1000]),\n",
    "          callbacks=[tensorboard_callback,model_ckpt])\n",
    "\n",
    "cnn_results = cmodel.evaluate(x_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7ab7de-6af4-479c-afc5-dbc7e418c2a5",
   "metadata": {},
   "source": [
    "### Analysis of classical layers\n",
    "The CNN model accomplishes the classification task incredibly well averaging 98% in performance. We can confirm the performance with the confusion matrix where the model predicted the hand written digits for all 10000 images in the test set. The state of the art classical image classification ML models can accomplish similar performances for incredibly complex images using customized feature extraction, trainable layers and other techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470feb6d-0295-461e-b2fc-b1ba1040b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cmodel.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba017d8b-d192-4751-9f5e-ff6657202942",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "plot_confusion_matrix(y_test, y_pred, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a4ebb-d647-449c-9534-76cdf2cdc3b3",
   "metadata": {},
   "source": [
    "Let's see how a quantum machine learning model stacks up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280ecde-148a-4319-9acf-1d85aa674bd2",
   "metadata": {},
   "source": [
    "# QML\n",
    "In this era, Parameterized Quantum Circuits (PQC), where gates consist of fixed and trained unitary operators which perform rotations on qubits, dominate in the implementation of QML algorithms. More specifically, PQC based QML algorithms currently involve seminal machine learning datasets such as MNIST, fashion-MNIST, CIFAR-10 to understand the performance of various architectures such as TTN, MERA, and ladder-lik. For MNIST classification, several of the aforementioned circuits have produced comparable or outperforming results for binary classification compared to their classical counterparts. The performances of the most recent models achieve 95% to 97% for 5 vs 3 classification and are consistently outperforming fair classical models. The strong binary classification results and consistent improvements since Farhi et al's binary classification, which achieved 85\\%, strongly suggesting that multi-classification is an exciting step in furthering understanding the expressivity of PQCs. Models such as those by Chalumuri et al. which achieves 92.10% on the IRIS dataset Bokhan et al. where their model classified 4 classes to acheive 85.14% and 90.03%, as well as Zeng et al. have shown exciting potential for QNN models in multiclassification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c645d08-a345-4a4f-8a7c-96584f38616a",
   "metadata": {},
   "source": [
    "## Quantum Computing and it's proposed benefits and limitations\n",
    "Using a quantum computer for a classical machine learning is still a very new and active area of research. In fact, this notebook should illustrate to you *why* there needs to be immense progress in hardware and software until a quantum computer is suitable for a classical image classifiation task. I personally found MNIST to be my hello world to ML, and hearing that there are some CS background individuals I hope that this notebook can be a hellow world to QML. The notebook isn't inteded to be a fun and wonderfully entry point for those with an inclination for machine learning to appreciate the exciting current challenges in QML.\n",
    "\n",
    "\n",
    "### Possible, theoretical, benefits:\n",
    "\n",
    "- Improving time complextities of linear algebra subroutines (singular value decomposition) using kernel methods.\n",
    "\n",
    "- Exponential time complexity speed ups \n",
    "\n",
    "- Overall there needs to be more understanding of how to frame the problem.\n",
    "\n",
    "https://quantumcomputing.stackexchange.com/questions/13531/what-is-the-advantage-of-quantum-machine-learning-over-traditional-machine-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71533f24-d6a5-407d-8feb-64b60103e1aa",
   "metadata": {},
   "source": [
    "### Current challenges\n",
    "\n",
    "These challenges are just some out of many which require further research\n",
    "\n",
    "#### 1: embedding and decoding classical data using quantum circuits\n",
    "\n",
    "During the NISQ era, there are severe limitations with how to encoding the images onto a quantum register for gate based implementations. For MNIST, the full image cannot be encoded into a quantum register; therefore, downsampling techniques are used to reduce the image size. In addition instead of implementing QRAM, embedding techniques such as angle and amplitude encodings are used to convert the classicla data to quantum data on the fly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd635b77-fb76-4dd3-9c74-ee8507b11f8c",
   "metadata": {},
   "source": [
    "#### 2: there aren't enough qubits, existing qubits are prone to error, and QRAM is not available.\n",
    "Current NISQ quantum computers are too noisy and have too few qubits for actually obtaining results. It's just as effective to use a noise model and a simulator to understand some of the quantum machine learning algorithms on hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a4772-3b0a-4a0f-83b3-f5b75dacb036",
   "metadata": {},
   "source": [
    "#### 3: cost of running circuits\n",
    "\n",
    "The cost of running an actual QML task on a quantum computer can be incredibly expensive $3 ~ 4k (USD) as of 2022. This is most apparent on the IonQ device and the accessibility of the devices are supported by credit programs by various groups such as AWS, IBM, and of course qBraid!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e90f47-de9f-4a4d-8cba-54711809f2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess the MNIST dataset for quantum computing\n",
    "For this demo, we will only apply a classical preprocessing technique suitable to different embedding techniques. It must be noted that angle embedding will require distorting the image to a shape of 8 x 4 for this simple demo. The classical preprocessing can, in fact, be customized a lot more to maximize the effectiveness of the embedding technique. Our demo is purely intended to show the current techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2b217-45c4-4f1d-a9a9-8de7c0f57a77",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "The preprocess will encode the image using 8 qubits and various embedding techniques. \n",
    "\n",
    "Angle embeddingl:\n",
    "1. Remove the 5 pixel border around the image which contains no data.\n",
    "2. Resize the image to 8 x 4.\n",
    "\n",
    "\n",
    "Aamplitude embedding:\n",
    "1. Remove the 5 pixel border around the image which contains no data.\n",
    "2. Resize the image to 16 x 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56897968-64e0-40b8-a4e0-fd59e55e848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop and remove border from image\n",
    "x_train_crop = np.array([x.reshape(28,28)[4:26,4:26] for x in x_train])\n",
    "x_test_crop =np.array([x.reshape(28,28)[4:26,3:26] for x in x_test])\n",
    "\n",
    "# Add channel dimension to dataset for resizing [ batch, height, width, channel]\n",
    "x_train_crop = x_train_crop[..., np.newaxis]\n",
    "x_test_crop = x_test_crop[..., np.newaxis]\n",
    "\n",
    "\n",
    "# Resize image to fit on register for angle embedding\n",
    "x_train_32 = tf.image.resize(x_train_crop, [8,8]).numpy()\n",
    "x_test_32 = tf.image.resize(x_test_crop, [8,8]).numpy()\n",
    "\n",
    "\n",
    "# Resize image to fit on register for amplitude embedding\n",
    "x_train_256 = tf.image.resize(x_train_crop, [16,16]).numpy()\n",
    "x_test_256 = tf.image.resize(x_test_crop, [16,16]).numpy()\n",
    "\n",
    "# Confirm dataset is still correct batch and shape\n",
    "print(f'The batch size is: {len(x_train_256)} images')\n",
    "print(f'Image height and width (angle): {x_train_256[10].shape} \\n')\n",
    "\n",
    "# Confirm dataset is still correct batch and shape\n",
    "print(f'The batch size is: {len(x_train_256)} images')\n",
    "print(f'Image height and width (amplitude): {x_train_256[10].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc609c0e-bfce-4bbc-b953-bf9351bae3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the results of the preprocessing for amplitude embedding\n",
    "plt.imshow(x_train_256[599])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2a3e0-dec8-43b4-8a7d-15cc8f453fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the results of the preprocessing for angle embedding\n",
    "plt.imshow(x_train_32[599])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f4d3d-1d1b-4fb1-8dbd-f19a69d8aea0",
   "metadata": {},
   "source": [
    "### 3 vs 5? Caveats to the image preprocessing:\n",
    "The downsampling can result in images that are so distorted that it may not be recognizable even by humans on what digit it represents. Let's just say that the preprocessing can produce some messy handwriting.\n",
    "\n",
    "<img src=\"./_images/3vs5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92f2f4-3574-4655-ab04-6325eadea123",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Methods of embedding the image into a circuit\n",
    "\n",
    "First we need to pre-process the data to ensure it can be embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b1b1a9-8de9-4f52-a024-b312fb2c8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants and universal functions\n",
    "WIRES=8\n",
    "weight_shapes = {\"weights\": (1,8,3) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d4af1-1f1e-4885-801b-8ada335477ca",
   "metadata": {},
   "source": [
    "#### Angle Embedding\n",
    "Angle embedding encodes the image data as rotations applied to the qubit. To fit the data on 8 qubits we will encode each of the pixels as Pauli Z rotations. The rotations will be applied for every row of the image.\n",
    "\n",
    "$$U_{\\text{angle encoding}} = Z^{N_i} \\mid n_i \\rangle$$\n",
    "\n",
    "<img src=\"./_images/angle_embedding.webp\">\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02734ba-3c8a-48db-83c5-3872721e7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will develop the embedding circuit\n",
    "dev = qml.device('default.qubit', wires=WIRES)\n",
    "@qml.qnode(dev)\n",
    "def angle_embedding(inputs):\n",
    "    # We apply a Pauli Z rotation to incode every pixel in the 8 x 8\n",
    "    for i in range(WIRES): \n",
    "        qml.AngleEmbedding(features=inputs[i].flatten(), wires=range(WIRES), rotation='Z')\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "188ca815-a30f-41d3-8a42-4a63cfd43ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 3)\n",
      "0: ─╭AngleEmbedding(M0)─╭AngleEmbedding(M1)─╭AngleEmbedding(M2)─╭AngleEmbedding(M3)\n",
      "1: ─├AngleEmbedding(M0)─├AngleEmbedding(M1)─├AngleEmbedding(M2)─├AngleEmbedding(M3)\n",
      "2: ─├AngleEmbedding(M0)─├AngleEmbedding(M1)─├AngleEmbedding(M2)─├AngleEmbedding(M3)\n",
      "3: ─├AngleEmbedding(M0)─├AngleEmbedding(M1)─├AngleEmbedding(M2)─├AngleEmbedding(M3)\n",
      "4: ─├AngleEmbedding(M0)─├AngleEmbedding(M1)─├AngleEmbedding(M2)─├AngleEmbedding(M3)\n",
      "5: ─├AngleEmbedding(M0)─├AngleEmbedding(M1)─├AngleEmbedding(M2)─├AngleEmbedding(M3)\n",
      "6: ─├AngleEmbedding(M0)─├AngleEmbedding(M1)─├AngleEmbedding(M2)─├AngleEmbedding(M3)\n",
      "7: ─╰AngleEmbedding(M0)─╰AngleEmbedding(M1)─╰AngleEmbedding(M2)─╰AngleEmbedding(M3)\n",
      "\n",
      "──╭AngleEmbedding(M4)─╭AngleEmbedding(M5)─╭AngleEmbedding(M6)─╭AngleEmbedding(M7)─┤  <Z>\n",
      "──├AngleEmbedding(M4)─├AngleEmbedding(M5)─├AngleEmbedding(M6)─├AngleEmbedding(M7)─┤     \n",
      "──├AngleEmbedding(M4)─├AngleEmbedding(M5)─├AngleEmbedding(M6)─├AngleEmbedding(M7)─┤     \n",
      "──├AngleEmbedding(M4)─├AngleEmbedding(M5)─├AngleEmbedding(M6)─├AngleEmbedding(M7)─┤     \n",
      "──├AngleEmbedding(M4)─├AngleEmbedding(M5)─├AngleEmbedding(M6)─├AngleEmbedding(M7)─┤     \n",
      "──├AngleEmbedding(M4)─├AngleEmbedding(M5)─├AngleEmbedding(M6)─├AngleEmbedding(M7)─┤     \n",
      "──├AngleEmbedding(M4)─├AngleEmbedding(M5)─├AngleEmbedding(M6)─├AngleEmbedding(M7)─┤     \n",
      "──╰AngleEmbedding(M4)─╰AngleEmbedding(M5)─╰AngleEmbedding(M6)─╰AngleEmbedding(M7)─┤     \n"
     ]
    }
   ],
   "source": [
    "shape = qml.StronglyEntanglingLayers.shape(n_layers=1, n_wires=8)\n",
    "weights = np.random.random(size=shape)\n",
    "print(weights.shape)\n",
    "print(qml.draw(angle_embedding)(x_train_32[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dd2ae-26a1-4ee2-b0ac-f982f16a2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create quantum layer which only contains the embedding strongly entangling layers \n",
    "used in the circuit centric classifer by Shuld et al. and the AMM readout strategy by Zeng et al.\n",
    "\"\"\"\n",
    "\n",
    "def amm_strategy(wires): \n",
    "    \"\"\"\n",
    "    All-qubit Multi-observable Measurement (AMM) strategy applies pauli operators \n",
    "    X, Y, Z on all qubits to extract features from disentangled quantum state\n",
    "    (Zeng et al.).\n",
    "    \"\"\"\n",
    "    readout = []\n",
    "    # Applies pauli\n",
    "    for i in range(8):\n",
    "        readout.append(qml.expval(qml.PauliX(i)))\n",
    "        readout.append(qml.expval(qml.PauliY(i)))\n",
    "        readout.append(qml.expval(qml.PauliZ(i)))            \n",
    "    return  readout\n",
    "    \n",
    "    \n",
    "@qml.qnode(dev)\n",
    "def angle_layer(inputs,weights):\n",
    "    \"\"\"\n",
    "    Quantum Angle embedding layer which will apply the embedding.\n",
    "    \"\"\"\n",
    "    for i in range(8): \n",
    "        qml.AngleEmbedding(features=tf.reshape(inputs[0],[-1]), wires=range(8), rotation='Z')\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(8))\n",
    "    return amm_strategy(wires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a986f1-8c50-4f81-9a6f-92d672608195",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlayer_angle = qml.qnn.KerasLayer(angle_layer, weight_shapes, output_dim=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d443f3c-493c-4d09-998d-5175083fc670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Keras model for angle embedding.\n",
    "qmodel_angle = tf.keras.Sequential([\n",
    "# The input is the data-circuit, encoded as a tf.string\n",
    "    qlayer_angle,\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494de6c2-332b-4bc9-9083-2114668520c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_angle.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    optimizer=tf.keras.optimizers.Adam(0.02))\n",
    "qmodel_angle.build(input_shape=[32,8,8,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b62d9-09af-462b-80f4-6b07b4fe4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_angle.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26951024-739c-4805-a73a-8e9ac5240539",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn_angle_history = qmodel_angle.fit(x_train_32[:1000],\n",
    "          y_train_onehot[:1000],\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test_32[:1000], y_test_onehot[:1000]),\n",
    "          callbacks=[tensorboard_callback,model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe625bd-98e2-404f-af86-f3e634074ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn_angle_results = qmodel_angle.evaluate(x_test_32, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0b6fc-0e33-4380-b8a8-cc4a881310d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_angle = qmodel_angle.predict(x_test_256)\n",
    "y_pred_angle = np.argmax(y_pred, axis=1)\n",
    "print(y_pred_amplitude, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d441d44-b592-4b2d-82ae-58ef6f850c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "plot_confusion_matrix(y_test, y_pred_angle, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b595c-96c2-428d-82d5-9e0fe08a6db6",
   "metadata": {},
   "source": [
    "#### Amplitude Embedding (wave function embedding)\n",
    "With amplitude embedding we will encode $2^n$ features where $n$ represents the number of qubits. In simple terms, the amplitude is the height of a wave. In this kind of embedding the data points are transformed into amplitudes of the quantum state. \n",
    "\n",
    "\n",
    "For example the classical data $[1,2,3,4]$ can be normalized and encoded on two qubits as so $$\\mid \\psi \\rangle = \\frac{1}{\\sqrt{30}} (\\mid 00 \\rangle + 2 \\mid 01 \\rangle + 3 \\mid 10 \\rangle + 4 \\mid 00 \\rangle)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa5375-1f37-49d9-98d4-7d6c115eab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will develop the embedding circuit\n",
    "dev = qml.device('default.qubit', wires=WIRES)\n",
    "@qml.qnode(dev)\n",
    "def amplitude_embedding(inputs):\n",
    "    qml.AmplitudeEmbedding(features=inputs, wires=range(WIRES), normalize=True)\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d819153-93e1-44c9-9bdb-de3e01d1bcaa",
   "metadata": {},
   "source": [
    "We can verify that our amplitude properly embeds an image into the circuit. The circuit encodes 256 pixels on 8 registers (wires) and returns a Z basis measurement of the first wire. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78484cd-36fa-4f3e-ae36-f9907134d9dd",
   "metadata": {},
   "source": [
    "##### What does this embedding strategy look like?\n",
    "By displaying the block sphere we see that the circuit is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bc6d196-d22b-4af6-9ceb-465ad72921e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def amplitude_layer(inputs,weights):\n",
    "    \"\"\"\n",
    "    Quantum amplitude encoding layer which will apply the embedding.\n",
    "    \"\"\"\n",
    "    qml.AmplitudeEmbedding(features=inputs, wires=range(WIRES), normalize=True)\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(WIRES))\n",
    "    return amm_strategy(wires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65d7d99e-35e9-4ae4-b8d1-85552df94b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'amm_strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom(size\u001b[38;5;241m=\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamplitude_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_256\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages/pennylane/drawer/draw.py:190\u001b[0m, in \u001b[0;36mdraw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     qnode\u001b[38;5;241m.\u001b[39mexpansion_strategy \u001b[38;5;241m=\u001b[39m expansion_strategy \u001b[38;5;129;01mor\u001b[39;00m original_expansion_strategy\n\u001b[0;32m--> 190\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m \u001b[43mqnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     qnode\u001b[38;5;241m.\u001b[39mexpansion_strategy \u001b[38;5;241m=\u001b[39m original_expansion_strategy\n",
      "File \u001b[0;32m~/.qbraid/environments/qbraid_sdk_9j9sjy/pyenv/lib/python3.9/site-packages/pennylane/qnode.py:525\u001b[0m, in \u001b[0;36mQNode.construct\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mQuantumTape()\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape:\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qfunc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39m_qfunc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qfunc_output\n\u001b[1;32m    528\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mget_parameters(trainable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [42], line 8\u001b[0m, in \u001b[0;36mamplitude_layer\u001b[0;34m(inputs, weights)\u001b[0m\n\u001b[1;32m      6\u001b[0m qml\u001b[38;5;241m.\u001b[39mAmplitudeEmbedding(features\u001b[38;5;241m=\u001b[39minputs, wires\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(WIRES), normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m qml\u001b[38;5;241m.\u001b[39mtemplates\u001b[38;5;241m.\u001b[39mStronglyEntanglingLayers(weights, wires\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(WIRES))\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mamm_strategy\u001b[49m(wires)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'amm_strategy' is not defined"
     ]
    }
   ],
   "source": [
    "shape = qml.StronglyEntanglingLayers.shape(n_layers=1, n_wires=WIRES)\n",
    "weights = np.random.random(size=shape)\n",
    "print(weights.shape)\n",
    "print(qml.draw(amplitude_layer)(x_train_256[0].flatten(), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9baab504-aeb9-48e4-8dd6-b206819010f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlayer_amplitude = qml.qnn.KerasLayer(amplitude_layer, weight_shapes, output_dim=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5b1ce-e38b-4f17-8d2f-572ac50f4549",
   "metadata": {},
   "source": [
    "The model we will use will only use the embedding circuit and the strongly entangling layers to verify the performance. We will keep the quantum layers to be the same for each of the embedding processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accaa1d8-15f1-498d-92f5-4f7ba7257667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Keras model for amplitude embedding.\n",
    "qmodel_amplitude = tf.keras.Sequential([\n",
    "# The input is the data-circuit, encoded as a tf.string\n",
    "    tf.keras.layers.Flatten(),\n",
    "    qlayer_amplitude,\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab5321-053e-4aac-820e-36c0718a21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_amplitude.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    optimizer=tf.keras.optimizers.Adam(0.02))\n",
    "qmodel_amplitude.build(input_shape=[32,16,16,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16547416-08df-4ada-aad8-9d8fa3082dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_amplitude.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1de66b-82e1-45ca-a615-0c0389dfb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn_history_amplitude = qmodel_amplitude.fit(x_train_256[:1000],\n",
    "          y_train_onehot[:1000],\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test_256[:1000], y_test_onehot[:1000]),\n",
    "          callbacks=[tensorboard_callback,model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38806e-ef84-48d9-9046-940de5edaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn_amplitude_results = qmodel_amplitude.evaluate(x_test_256, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4468be-7a10-4582-9931-b093d0536f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_amplitude = qmodel_amplitude.predict(x_test_256)\n",
    "y_pred_amplitude = np.argmax(y_pred, axis=1)\n",
    "print(y_pred_amplitude, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d53dd9-4611-45ba-81c6-12a184ba67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "plot_confusion_matrix(y_test, y_pred_amplitude, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5e026-febd-4430-8bbc-6872c2bdda62",
   "metadata": {},
   "source": [
    "#### Full model results for various papers\n",
    "\n",
    "<img src=\"./_images/10_class.png\" width=\"450px\">\n",
    "\n",
    "There are plenty of amazing examples of work that are achieving improvements in training PQC for MNIST image classification. Here's one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41f852-9e21-4811-88fa-5798b17b7954",
   "metadata": {},
   "source": [
    "<img src=\"./_images/training_final.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba0f3a-6b61-459e-b9cc-fa7e5cd51bbb",
   "metadata": {},
   "source": [
    "The sectional angle embedding technique which allows for minimal\n",
    "downsampling of classical MNIST image dataset using 10 qubits and averaging\n",
    "to 85.32% over 10 trials is demonstrated. In doing so, the HQNN results in com-\n",
    "parable performances of 89.09% by Zeng et al.[1] for 10 class multi-classification.\n",
    "While optimistic performances were achieved, sectioned angle embedding allows\n",
    "for a desired number of sections embedded on single qubit gates suggesting further improvements to be explored to reduce the qubit count and improve the image granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e9330-e962-452e-a8bb-1ffac8492ce3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running a fair model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b6af9-1172-422b-bd74-e7dbfcfee970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Keras model.\n",
    "fair_model = tf.keras.Sequential([\n",
    "# The input is the data-circuit, encoded as a tf.string\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df5e05-2ef4-4888-8bec-aac387b85203",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    optimizer=tf.keras.optimizers.Adam(0.02))\n",
    "fair_model.build(input_shape=[32,16,16,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd57398-0ffa-40cb-984c-cb9f99f823db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479b256-e724-4412-a2bf-91fc4ce7b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_model_history = fair_model.fit(x_train_256[:1000],\n",
    "          y_train_onehot[:1000],\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test_256[:1000], y_test_onehot[:1000]),\n",
    "          callbacks=[tensorboard_callback,model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9e34a-e3f3-4f95-b3d9-cdd2ddf960cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_results = fair_model.evaluate(x_test_256, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a3412-502e-4f09-a5b3-8483e43ebedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fair_pred = fair_model.predict(x_test_256)\n",
    "y_fair_pred = np.argmax(y_fair_pred, axis=1)\n",
    "print(y_test, y_fair_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7bb3b-198f-440c-9d20-1f5a995ee641",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "plot_confusion_matrix(y_test, y_fair_pred, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267c793-85a7-4e1b-aa88-ab217ba3ba42",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Using the qBraid SDK to transpile and run on multiple devices\n",
    "\n",
    "<img src=\"./_images/qbraid_logo_white_large.png\" width=100px>\n",
    "The qBraid SDK is a Python toolkit for building and executing quantum programs.\n",
    "\n",
    "\n",
    "Features:\n",
    "\n",
    "- Unified quantum frontend interface. Transpile quantum circuits between supported packages. Leverage the capabilities of multiple frontends through simple, consistent protocols.\n",
    "- Build once, target many. Create quantum programs using your preferred circuit-building package, and execute on any backend that interfaces with a supported frontend.\n",
    "- Benchmark, compare, interpret results. Built-in compatible post-processing enables comparing results between runs and across backends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65610150-33eb-4d93-bf64-1d69b4bfae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "We will create the angle embedding as a QML Tape which records\n",
    "and stores variational quantum programs.\n",
    "\"\"\"\n",
    "with qml.tape.QuantumTape() as tape:\n",
    "    # Apply angle embedding to 1 image\n",
    "    angle_embedding(x_train_32[0])\n",
    "    \n",
    "# Apply qbraid sdk circuit wrapper\n",
    "pennylane_circuit = circuit_wrapper(tape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3113ecc-da51-4193-892b-7d4c922e46f5",
   "metadata": {},
   "source": [
    "Once we have wrapped the circuit, we will now transpile the circuit to various other frameworks and see what the circuit looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8718265a-a831-42d2-a063-e8fd38472a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cirq Circuit\n",
      "pyquil Program\n",
      "qiskit QuantumCircuit\n",
      "braket Circuit\n",
      "pennylane QuantumTape\n"
     ]
    }
   ],
   "source": [
    "from qbraid import SUPPORTED_PROGRAM_TYPES\n",
    "for _, (k, v) in enumerate(SUPPORTED_PROGRAM_TYPES.items()):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5bfd41e5-fec7-4a7f-935f-b0f0302bd437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_0: ───Rz(0)─────────Rz(0)─────────Rz(0)─────────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(0)─────────Rz(-1.61π)────M('c_0')───\n",
      "\n",
      "q_1: ───Rz(0)─────────Rz(0.522π)────Rz(0)─────────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(-1.15π)────Rz(-0.671π)──────────────\n",
      "\n",
      "q_2: ───Rz(0)─────────Rz(0.532π)────Rz(1.84π)─────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(-0.441π)───Rz(0.649π)───────────────\n",
      "\n",
      "q_3: ───Rz(1.01π)─────Rz(-0.835π)───Rz(-0.459π)───Rz(-1.42π)───Rz(0.224π)───Rz(0.144π)────Rz(-0.107π)───Rz(0)────────────────────\n",
      "\n",
      "q_4: ───Rz(-0.666π)───Rz(1.72π)─────Rz(0)─────────Rz(1.96π)────Rz(0.955π)───Rz(-0.614π)───Rz(-0.842π)───Rz(0)────────────────────\n",
      "\n",
      "q_5: ───Rz(-1.2π)─────Rz(-1.72π)────Rz(0)─────────Rz(0.373π)───Rz(1.89π)────Rz(0.049π)────Rz(0)─────────Rz(0)────────────────────\n",
      "\n",
      "q_6: ───Rz(-1.81π)────Rz(1.92π)─────Rz(0)─────────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(0)─────────Rz(0)────────────────────\n",
      "\n",
      "q_7: ───Rz(0)─────────Rz(0)─────────Rz(0)─────────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(0)─────────Rz(0)────────────────────\n"
     ]
    }
   ],
   "source": [
    "braket_circuit = pennylane_circuit.transpile(\"cirq\")\n",
    "print(braket_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12bc9c12-6d88-4fac-817e-37c844cdb31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_0: ───Rz(0)─────────Rz(0)─────────Rz(0)─────────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(0)─────────Rz(-1.61π)────M('c_0')───\n",
      "\n",
      "q_1: ───Rz(0)─────────Rz(0.522π)────Rz(0)─────────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(-1.15π)────Rz(-0.671π)──────────────\n",
      "\n",
      "q_2: ───Rz(0)─────────Rz(0.532π)────Rz(1.84π)─────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(-0.441π)───Rz(0.649π)───────────────\n",
      "\n",
      "q_3: ───Rz(1.01π)─────Rz(-0.835π)───Rz(-0.459π)───Rz(-1.42π)───Rz(0.224π)───Rz(0.144π)────Rz(-0.107π)───Rz(0)────────────────────\n",
      "\n",
      "q_4: ───Rz(-0.666π)───Rz(1.72π)─────Rz(0)─────────Rz(1.96π)────Rz(0.955π)───Rz(-0.614π)───Rz(-0.842π)───Rz(0)────────────────────\n",
      "\n",
      "q_5: ───Rz(-1.2π)─────Rz(-1.72π)────Rz(0)─────────Rz(0.373π)───Rz(1.89π)────Rz(0.049π)────Rz(0)─────────Rz(0)────────────────────\n",
      "\n",
      "q_6: ───Rz(-1.81π)────Rz(1.92π)─────Rz(0)─────────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(0)─────────Rz(0)────────────────────\n",
      "\n",
      "q_7: ───Rz(0)─────────Rz(0)─────────Rz(0)─────────Rz(0)────────Rz(0)────────Rz(0)─────────Rz(0)─────────Rz(0)────────────────────\n"
     ]
    }
   ],
   "source": [
    "cirq_circuit = circuit_wrapper(braket_circuit).transpile(\"cirq\")\n",
    "print(cirq_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8796d6d2-2a22-4c89-a4d8-dea01c7ad3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">           ┌───────┐     ┌───────┐     ┌───────┐     ┌───────┐   »\n",
       "    q_0: ──┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├───»\n",
       "         ┌─┴───────┴─┐ ┌─┴───────┴──┐  ├───────┤     ├───────┤   »\n",
       "    q_1: ┤ Rz(220.5) ├─┤ Rz(18.609) ├──┤ Rz(0) ├─────┤ Rz(0) ├───»\n",
       "         ├───────────┴┐├────────────┤  ├───────┤   ┌─┴───────┴──┐»\n",
       "    q_2: ┤ Rz(71.641) ├┤ Rz(32.297) ├──┤ Rz(0) ├───┤ Rz(1.1719) ├»\n",
       "         ├────────────┤├────────────┤  ├───────┤   ├────────────┤»\n",
       "    q_3: ┤ Rz(98.437) ├┤ Rz(244.17) ├──┤ Rz(0) ├───┤ Rz(131.81) ├»\n",
       "         ├───────────┬┘├───────────┬┘┌─┴───────┴──┐├────────────┤»\n",
       "    q_4: ┤ Rz(15.75) ├─┤ Rz(248.7) ├─┤ Rz(111.66) ├┤ Rz(221.73) ├»\n",
       "         └─┬───────┬─┘ └┬─────────┬┘ ├────────────┤└─┬───────┬──┘»\n",
       "    q_5: ──┤ Rz(0) ├────┤ Rz(253) ├──┤ Rz(56.031) ├──┤ Rz(0) ├───»\n",
       "           ├───────┤   ┌┴─────────┴─┐└─┬───────┬──┘  ├───────┤   »\n",
       "    q_6: ──┤ Rz(0) ├───┤ Rz(152.44) ├──┤ Rz(0) ├─────┤ Rz(0) ├───»\n",
       "           ├───────┤   └─┬───────┬──┘  ├───────┤     ├───────┤   »\n",
       "    q_7: ──┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├───»\n",
       "           └───────┘     └───────┘     └───────┘     └───────┘   »\n",
       "m_c_0: 1/════════════════════════════════════════════════════════»\n",
       "                                                                 »\n",
       "«            ┌───────┐     ┌───────┐     ┌───────┐     ┌───────┐      \n",
       "«    q_0: ───┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├──────\n",
       "«            ├───────┤     ├───────┤     ├───────┤     ├───────┤      \n",
       "«    q_1: ───┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├──────\n",
       "«          ┌─┴───────┴──┐┌─┴───────┴──┐  ├───────┤     ├───────┤      \n",
       "«    q_2: ─┤ Rz(194.42) ├┤ Rz(213.78) ├──┤ Rz(0) ├─────┤ Rz(0) ├──────\n",
       "«          ├────────────┤├────────────┤┌─┴───────┴──┐  ├───────┤      \n",
       "«    q_3: ─┤ Rz(103.53) ├┤ Rz(224.27) ├┤ Rz(60.188) ├──┤ Rz(0) ├──────\n",
       "«         ┌┴────────────┤├────────────┤├────────────┤  ├───────┤      \n",
       "«    q_4: ┤ Rz(0.70312) ├┤ Rz(100.98) ├┤ Rz(225.86) ├──┤ Rz(0) ├──────\n",
       "«         └──┬───────┬──┘└─┬───────┬──┘├────────────┤┌─┴───────┴──┐   \n",
       "«    q_5: ───┤ Rz(0) ├─────┤ Rz(0) ├───┤ Rz(237.38) ├┤ Rz(77.438) ├───\n",
       "«            ├───────┤     ├───────┤   ├────────────┤├────────────┤   \n",
       "«    q_6: ───┤ Rz(0) ├─────┤ Rz(0) ├───┤ Rz(84.359) ├┤ Rz(198.95) ├───\n",
       "«            ├───────┤     ├───────┤   └─┬───────┬──┘├────────────┤┌─┐\n",
       "«    q_7: ───┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├───┤ Rz(208.58) ├┤M├\n",
       "«            └───────┘     └───────┘     └───────┘   └────────────┘└╥┘\n",
       "«m_c_0: 1/══════════════════════════════════════════════════════════╩═\n",
       "«                                                                   0 </pre>"
      ],
      "text/plain": [
       "           ┌───────┐     ┌───────┐     ┌───────┐     ┌───────┐   »\n",
       "    q_0: ──┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├───»\n",
       "         ┌─┴───────┴─┐ ┌─┴───────┴──┐  ├───────┤     ├───────┤   »\n",
       "    q_1: ┤ Rz(220.5) ├─┤ Rz(18.609) ├──┤ Rz(0) ├─────┤ Rz(0) ├───»\n",
       "         ├───────────┴┐├────────────┤  ├───────┤   ┌─┴───────┴──┐»\n",
       "    q_2: ┤ Rz(71.641) ├┤ Rz(32.297) ├──┤ Rz(0) ├───┤ Rz(1.1719) ├»\n",
       "         ├────────────┤├────────────┤  ├───────┤   ├────────────┤»\n",
       "    q_3: ┤ Rz(98.437) ├┤ Rz(244.17) ├──┤ Rz(0) ├───┤ Rz(131.81) ├»\n",
       "         ├───────────┬┘├───────────┬┘┌─┴───────┴──┐├────────────┤»\n",
       "    q_4: ┤ Rz(15.75) ├─┤ Rz(248.7) ├─┤ Rz(111.66) ├┤ Rz(221.73) ├»\n",
       "         └─┬───────┬─┘ └┬─────────┬┘ ├────────────┤└─┬───────┬──┘»\n",
       "    q_5: ──┤ Rz(0) ├────┤ Rz(253) ├──┤ Rz(56.031) ├──┤ Rz(0) ├───»\n",
       "           ├───────┤   ┌┴─────────┴─┐└─┬───────┬──┘  ├───────┤   »\n",
       "    q_6: ──┤ Rz(0) ├───┤ Rz(152.44) ├──┤ Rz(0) ├─────┤ Rz(0) ├───»\n",
       "           ├───────┤   └─┬───────┬──┘  ├───────┤     ├───────┤   »\n",
       "    q_7: ──┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├───»\n",
       "           └───────┘     └───────┘     └───────┘     └───────┘   »\n",
       "m_c_0: 1/════════════════════════════════════════════════════════»\n",
       "                                                                 »\n",
       "«            ┌───────┐     ┌───────┐     ┌───────┐     ┌───────┐      \n",
       "«    q_0: ───┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├──────\n",
       "«            ├───────┤     ├───────┤     ├───────┤     ├───────┤      \n",
       "«    q_1: ───┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├──────\n",
       "«          ┌─┴───────┴──┐┌─┴───────┴──┐  ├───────┤     ├───────┤      \n",
       "«    q_2: ─┤ Rz(194.42) ├┤ Rz(213.78) ├──┤ Rz(0) ├─────┤ Rz(0) ├──────\n",
       "«          ├────────────┤├────────────┤┌─┴───────┴──┐  ├───────┤      \n",
       "«    q_3: ─┤ Rz(103.53) ├┤ Rz(224.27) ├┤ Rz(60.188) ├──┤ Rz(0) ├──────\n",
       "«         ┌┴────────────┤├────────────┤├────────────┤  ├───────┤      \n",
       "«    q_4: ┤ Rz(0.70312) ├┤ Rz(100.98) ├┤ Rz(225.86) ├──┤ Rz(0) ├──────\n",
       "«         └──┬───────┬──┘└─┬───────┬──┘├────────────┤┌─┴───────┴──┐   \n",
       "«    q_5: ───┤ Rz(0) ├─────┤ Rz(0) ├───┤ Rz(237.38) ├┤ Rz(77.438) ├───\n",
       "«            ├───────┤     ├───────┤   ├────────────┤├────────────┤   \n",
       "«    q_6: ───┤ Rz(0) ├─────┤ Rz(0) ├───┤ Rz(84.359) ├┤ Rz(198.95) ├───\n",
       "«            ├───────┤     ├───────┤   └─┬───────┬──┘├────────────┤┌─┐\n",
       "«    q_7: ───┤ Rz(0) ├─────┤ Rz(0) ├─────┤ Rz(0) ├───┤ Rz(208.58) ├┤M├\n",
       "«            └───────┘     └───────┘     └───────┘   └────────────┘└╥┘\n",
       "«m_c_0: 1/══════════════════════════════════════════════════════════╩═\n",
       "«                                                                   0 "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qiskit_circuit = circuit_wrapper(cirq_circuit).transpile(\"qiskit\")\n",
    "qiskit_circuit.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a06bdab-6506-4ed6-9c38-084c29861498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECLARE m0 BIT[1]\n",
      "RZ(0) 0\n",
      "RZ(220.5) 1\n",
      "RZ(71.640625) 2\n",
      "RZ(98.4375) 3\n",
      "RZ(15.75) 4\n",
      "RZ(0) 5\n",
      "RZ(0) 6\n",
      "RZ(0) 7\n",
      "RZ(0) 0\n",
      "RZ(18.609375) 1\n",
      "RZ(32.296875) 2\n",
      "RZ(244.17188000000002) 3\n",
      "RZ(248.70312) 4\n",
      "RZ(252.99999999999997) 5\n",
      "RZ(152.4375) 6\n",
      "RZ(0) 7\n",
      "RZ(0) 0\n",
      "RZ(0) 1\n",
      "RZ(0) 2\n",
      "RZ(0) 3\n",
      "RZ(111.65625000000001) 4\n",
      "RZ(56.03125000000001) 5\n",
      "RZ(0) 6\n",
      "RZ(0) 7\n",
      "RZ(0) 0\n",
      "RZ(0) 1\n",
      "RZ(1.171875) 2\n",
      "RZ(131.8125) 3\n",
      "RZ(221.73438) 4\n",
      "RZ(0) 5\n",
      "RZ(0) 6\n",
      "RZ(0) 7\n",
      "RZ(0) 0\n",
      "RZ(0) 1\n",
      "RZ(194.42188) 2\n",
      "RZ(103.53124999999999) 3\n",
      "RZ(0.703125) 4\n",
      "RZ(0) 5\n",
      "RZ(0) 6\n",
      "RZ(0) 7\n",
      "RZ(0) 0\n",
      "RZ(0) 1\n",
      "RZ(213.78125000000003) 2\n",
      "RZ(224.26562) 3\n",
      "RZ(100.98438000000002) 4\n",
      "RZ(0) 5\n",
      "RZ(0) 6\n",
      "RZ(0) 7\n",
      "RZ(0) 0\n",
      "RZ(0) 1\n",
      "RZ(0) 2\n",
      "RZ(60.1875) 3\n",
      "RZ(225.85938) 4\n",
      "RZ(237.37499999999997) 5\n",
      "RZ(84.359375) 6\n",
      "RZ(0) 7\n",
      "RZ(0) 0\n",
      "RZ(0) 1\n",
      "RZ(0) 2\n",
      "RZ(0) 3\n",
      "RZ(0) 4\n",
      "RZ(77.4375) 5\n",
      "RZ(198.95312) 6\n",
      "RZ(208.57812) 7\n",
      "MEASURE 7 m0[0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyquil_circuit = circuit_wrapper(qiskit_circuit).transpile(\"pyquil\")\n",
    "print(pyquil_circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfbef8-21d5-4162-98c3-ff9898abfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyquil_circuit = circuit_wrapper(qiskit_circuit).transpile(\"pyquil\")\n",
    "print(pyquil_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a9c45-4b56-482a-a3dd-3cb9f297972b",
   "metadata": {},
   "source": [
    "# Citations\n",
    "1. Zeng Y, Wang H, He J, Huang Q, Chang S. \"A Multi-Classification Hybrid Quan-\n",
    "tum Neural Network Using an All-Qubit Multi-Observable Measurement Strategy.\"\n",
    "Entropy. 2022; 24(3):394. https://doi.org/10.3390/e24030394\n",
    "2. Farhi, Edward, and Hartmut Neven. \"Classification with quantum neural networks\n",
    "on near term processors.\" arXiv preprint arXiv:1802.06002 (2018).https://doi.\n",
    "org/10.48550/arXiv.1802.06002\n",
    "3. Bokhan, Denis, Alena S. Mastiukova, Aleksey S. Boev, Dmitrii N. Trubnikov, and\n",
    "Aleksey K. Fedorov. \"Multiclass classification using quantum convolutional neural\n",
    "networks with hybrid quantum-classical learning.\" arXiv preprint arXiv:2203.15368\n",
    "(2022).\n",
    "4. Chalumuri, A., Kune, R. Manoj, B.S. A hybrid classical-quantum approach for\n",
    "multi-class classification. Quantum Inf Process 20, 119 (2021).https://doi.org/\n",
    "10.1007/s11128-021-03029-9\n",
    "5. Feynman, R.P. Simulating physics with computers. Int J Theor Phys 21, 467–488\n",
    "(1982). https://doi.org/10.1007/BF02650179\n",
    "6. Biamonte, Jacob, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe,\n",
    "and Seth Lloyd. \"Quantum machine learning.\" Nature 549, no. 7671 (2017): 195-202.\n",
    "7. Huggins, William, Piyush Patil, Bradley Mitchell, K. Birgitta Whaley, and E. Miles\n",
    "Stoudenmire. \"Towards quantum machine learning with tensor networks.\" Quantum\n",
    "Science and technology 4, no. 2 (2019): 024001.\n",
    "8. Oh, Seunghyeok, Jaeho Choi, and Joongheon Kim. \"A tutorial on quantum convolu-\n",
    "tional neural networks (QCNN).\" In 2020 International Conference on Information\n",
    "and Communication Technology Convergence (ICTC), pp. 236-239. IEEE, 2020.\n",
    "9. Jiang, W., Xiong, J. Shi, Y. A co-design framework of neural networks and quantum\n",
    "circuits towards quantum advantage. Nat Commun 12, 579 (2021). https://doi.\n",
    "org/10.1038/s41467-020-20729-5\n",
    "10. Hellstern, G.: Analysis of a hybrid quantum network for classification tasks. IET\n",
    "Quant. Comm. 2( 4), 153– 159 (2021). https://doi.org/10.1049/qtc2.12017\n",
    "11. Arunachalam, Srinivasan, and Ronald de Wolf. \"Guest column: A survey of quan-\n",
    "tum learning theory.\" ACM Sigact News 48, no. 2 (2017): 41-67.\n",
    "12. Schuld, Maria, and Nathan Killoran. \"Is quantum advantage the right\n",
    "goal for quantum machine learning?.\" arXiv preprint arXiv:2203.01340 (2022).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643d8a0-1a44-47c1-936a-8aff6706c181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a95053-c30b-4e9a-b25c-f340a586eca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab1404-7eb5-4724-9227-d63448b5516b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [qBraid-SDK]",
   "language": "python",
   "name": "python3_qbraid_sdk_9j9sjy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
